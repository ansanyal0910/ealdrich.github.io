
<!DOCTYPE html>


<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Maximum Likelihood Estimation &mdash; Econ 114 - Advanced Quantitative Methods</title>
    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/styles.css" type="text/css" />
    <link rel="stylesheet" href="_static/single.css" type="text/css" />
    
    <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    

    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="top" title="Econ 114 - Advanced Quantitative Methods" href="index.html" />
    <link rel="up" title="Distributions" href="distributions.html" />
    <link rel="next" title="Resampling" href="resample.html" />
    <link rel="prev" title="Heavy-Tailed Distributions" href="heavyDist.html" /> 
  </head>
  <body>

<section
   id="slide_container"
   class='slides layout-regular'>


  <article class="appear slide level-1" id="maximum-likelihood-estimation">
<h1>Maximum Likelihood Estimation</h1>

<div class="slide-no">1</div>

</article>
<article class="appear slide level-2" id="estimating-parameters-of-distributions">
<h2>Estimating Parameters of Distributions</h2>
<p>We almost never know the true distribution of a data sample.</p>
<ul class="to-build simple">
<li>We might hypothesize a family of distributions that capture broad
characteristics of the data (locations, scale and shape).</li>
</ul>
<ul class="to-build simple">
<li>However, there may be a set of one or more parameters of the
distribution that we don't know.</li>
</ul>
<ul class="to-build simple">
<li>Typically we use the data to estimate the unknown parameters.</li>
</ul>

<div class="slide-no">2</div>

</article>
<article class="appear slide level-2" id="joint-densities">
<h2>Joint Densities</h2>
<p>Suppose we have a collection of random variables <span class="math">\({\bf Y} =
(Y_1, \ldots, Y_n)'\)</span>.</p>
<ul class="to-build simple">
<li>We view a data sample of size <span class="math">\(n\)</span> as one realization of each
random variable: <span class="math">\({\bf y} = (y_1, \ldots, y_n)'\)</span>.</li>
</ul>
<ul class="to-build simple">
<li>The <em>joint cumulative density</em> of <span class="math">\({\bf Y}\)</span> is</li>
</ul>
<div class="to-build math">
\[\begin{split}F_{{\bf Y}}({\bf y}) &amp; = P(Y_1 \leq y_1, \ldots, Y_n \leq
y_n).\end{split}\]</div>

<div class="slide-no">3</div>

</article>
<article class="appear slide level-2" id="id1">
<h2>Joint Densities</h2>
<ul class="simple">
<li>The <em>joint probability density</em> of <span class="math">\({\bf Y}\)</span> is</li>
</ul>
<div class="math">
\[\begin{split}f_{{\bf Y}}({\bf y}) &amp; = \frac{\partial^n
F_{{\bf Y}}({\bf y})}{\partial Y_1 \ldots \partial Y_n}.\end{split}\]</div>
<p class="to-build">since</p>
<div class="to-build math">
\[\begin{split}F_{{\bf Y}}({\bf y})&amp; = \int_{-\infty}^{y_1}\ldots
\int_{-\infty}^{y_n} f_{{\bf Y}}({\bf a}) \,\, da_1 \ldots
da_n.\end{split}\]</div>

<div class="slide-no">4</div>

</article>
<article class="appear slide level-2" id="independence">
<h2>Independence</h2>
<p>When <span class="math">\(Y_1, \ldots, Y_n\)</span> are independent of each other and have
identical distributions:</p>
<ul class="to-build simple">
<li>We say that they are <em>independent and identically distributed</em>, or
i.i.d.</li>
</ul>
<ul class="to-build simple">
<li>When <span class="math">\(Y_1, \ldots, Y_n\)</span> are i.i.d., they have the same
marginal densities:</li>
</ul>
<div class="to-build math">
\[\begin{split}f_{Y_1}(y) &amp; = \ldots = f_{Y_n}(y).\end{split}\]</div>

<div class="slide-no">5</div>

</article>
<article class="appear slide level-2" id="id2">
<h2>Independence</h2>
<p>Further, when <span class="math">\(Y_1, \ldots, Y_n\)</span> are i.i.d.</p>
<div class="math">
\[\begin{split}f_{{\bf Y}}({\bf y}) &amp; = f_{Y_1}(y_1) \cdot f_{Y_2}(y_2)
\cdots f_{Y_n}(y_n) = \prod_{i=1}^n f_{Y_i}(y_i).\end{split}\]</div>
<ul class="to-build simple">
<li>This is analogous to the computation of joint probabilities.</li>
</ul>
<ul class="to-build simple">
<li>For independent events <span class="math">\(A\)</span>, <span class="math">\(B\)</span> and <span class="math">\(C\)</span>,</li>
</ul>
<div class="to-build math">
\[\begin{split}P(A \cap B \cap C) &amp; = P(A)P(B)P(C).\end{split}\]</div>

<div class="slide-no">6</div>

</article>
<article class="appear slide level-2" id="id3">
<h2>Maximum Likelihood Estimation</h2>
<p>One of the most important and powerful methods of parameter estimation
is <em>maximum likelihood estimation</em>.  It requires</p>
<ul class="to-build simple">
<li>A data sample: <span class="math">\({\bf y} = (y_1, \ldots, y_n)'\)</span>.</li>
</ul>
<ul class="to-build simple">
<li>A joint probability density:</li>
</ul>
<div class="to-build math">
\[\begin{split}f_{{\bf Y}}({\bf y}|{\bf \theta}) &amp; = \prod_{i=1}^n
f_{Y_i}(y_i|{\bf \theta}).\end{split}\]</div>
<p class="to-build">where <span class="math">\({\bf \theta}\)</span> is a vector of parameters.</p>

<div class="slide-no">7</div>

</article>
<article class="appear slide level-2" id="likelihood">
<h2>Likelihood</h2>
<p><span class="math">\(f_{{\bf Y}}({\bf y}|{\bf \theta})\)</span> is loosely interpreted
as the probability of observing data sample <span class="math">\({\bf y}\)</span>, given a
functional form for the density of <span class="math">\(Y_1, \ldots, Y_n\)</span> and given
a set of parameters <span class="math">\({\bf \theta}\)</span>.</p>
<ul class="to-build simple">
<li>We can reverse the notion and think of <span class="math">\({\bf y}\)</span> as being
fixed and <span class="math">\({\bf \theta}\)</span> some unknown variable.</li>
</ul>
<ul class="to-build simple">
<li>In this case we write <span class="math">\(\mathcal{L}({\bf \theta}|{\bf y}) =
f_{{\bf Y}}({\bf y}|{\bf \theta})\)</span>.</li>
</ul>
<ul class="to-build simple">
<li>We refer to <span class="math">\(\mathcal{L}({\bf \theta}|{\bf y})\)</span> as the
likelihood.</li>
</ul>
<ul class="to-build simple">
<li>Fixing <span class="math">\({\bf y}\)</span>, maximum likelihood estimation chooses the
value of <span class="math">\({\bf \theta}\)</span> that maximizes
<span class="math">\(\mathcal{L}({\bf \theta}|{\bf y}) =
f_{{\bf Y}}({\bf y}|{\bf \theta})\)</span>.</li>
</ul>

<div class="slide-no">8</div>

</article>
<article class="appear slide level-2" id="likelihood-maximization">
<h2>Likelihood Maximization</h2>
<p>Given <span class="math">\({\bf \theta} = (\theta_1, \ldots, \theta_p)'\)</span>, we
maximize <span class="math">\(\mathcal{L}({\bf \theta}|{\bf y})\)</span> by</p>
<ul class="to-build simple">
<li>Differentiating with respect to each <span class="math">\(\theta_i\)</span>, <span class="math">\(i = 1,
\ldots, p\)</span>.</li>
</ul>
<ul class="to-build simple">
<li>Setting the resulting derivatives equal to zero.</li>
</ul>
<ul class="to-build simple">
<li>Solving for the values <span class="math">\(\hat{\theta}_i\)</span>, <span class="math">\(i = 1, \ldots,
p\)</span>, that make all of the derivatives zero.</li>
</ul>

<div class="slide-no">9</div>

</article>
<article class="appear slide level-2" id="log-likelihood">
<h2>Log Likelihood</h2>
<p>It is often easier to work with the logarithm of the likelihood
function.</p>
<ul class="to-build simple">
<li>By the properties of logarithms</li>
</ul>
<div class="to-build math">
\[\begin{split}\ell({\bf \theta}|{\bf y}) &amp; =
\log\left(\mathcal{L}({\bf \theta}|{\bf y})\right)\end{split}\]</div>
<div class="to-build math">
\[\begin{split}\qquad \enspace &amp; = \log \left(f_{{\bf Y}}({\bf y}|{\bf \theta})\right)\end{split}\]</div>
<div class="to-build math">
\[\begin{split}\qquad \enspace \qquad &amp; = \log \left(\prod_{i=1}^n
f_{Y_i}(y_i|{\bf \theta})\right)\end{split}\]</div>
<div class="to-build math">
\[\begin{split}\qquad \qquad &amp; = \sum_{i=1}^n \log\left(f_{Y_i}(y_i|{\bf
\theta})\right).\end{split}\]</div>

<div class="slide-no">10</div>

</article>
<article class="appear slide level-2" id="id4">
<h2>Log Likelihood</h2>
<ul class="simple">
<li>Maximizing <span class="math">\(\ell({\bf \theta}|{\bf y})\)</span> is the same as
maximizing <span class="math">\(\mathcal{L}({\bf \theta}|{\bf y})\)</span> since
<span class="math">\(\log\)</span> is a monotonic transformation.</li>
</ul>
<ul class="to-build simple">
<li>A derivative of <span class="math">\(\mathcal{L}\)</span> will involve many chain-rule
products, whereas a derivative of <span class="math">\(\ell\)</span> will simply be a sum
of derivatives.</li>
</ul>

<div class="slide-no">11</div>

</article>
<article class="appear slide level-2" id="mle-example">
<h2>MLE Example</h2>
<p>Suppose we have a dataset <span class="math">\({\bf y} = (y_1, \ldots, y_n)\)</span>,
where <span class="math">\(Y_1, \ldots, Y_n \stackrel{i.i.d.}{\sim} \mathcal{N}(\mu,
\sigma^2)\)</span>.</p>
<ul class="to-build simple">
<li>We will assume <span class="math">\(\mu\)</span> is <em>unknown</em> and <span class="math">\(\sigma\)</span> is
<em>known</em>.</li>
</ul>
<ul class="to-build simple">
<li>So, <span class="math">\({\bf \theta} = \mu\)</span> (it is a single value, rather than
a vector).</li>
</ul>

<div class="slide-no">12</div>

</article>
<article class="appear slide level-2" id="id5">
<h2>MLE Example</h2>
<ul class="simple">
<li>The likelihood is</li>
</ul>
<div class="math">
\[\begin{split}\mathcal{L}(\mu|{\bf y}) &amp; = f_{{\bf Y}}({\bf y}|\mu) \qquad \qquad
\qquad \qquad \qquad \qquad\end{split}\]</div>
<div class="to-build math">
\[\begin{split}&amp; = \prod_{i=1}^n f_{Y_i}(y_i|\mu) \qquad \qquad \qquad \qquad\end{split}\]</div>
<div class="to-build math">
\[\begin{split}\quad &amp; = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp
\left\{-\frac{1}{2} \frac{(y_i - \mu)^2}{\sigma^2} \right\}\end{split}\]</div>
<div class="to-build math">
\[\begin{split}\qquad \enspace &amp; = \frac{1}{(2\pi \sigma^2)^{n/2}} \exp
\left\{-\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \mu)^2 \right\}.\end{split}\]</div>

<div class="slide-no">13</div>

</article>
<article class="appear slide level-2" id="id6">
<h2>MLE Example</h2>
<p>The log likelihood is</p>
<div class="math">
\[\begin{split}\ell(\mu|{\bf y}) &amp; = -\frac{n}{2} \log(2\pi) - \frac{n}{2}
\log(\sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \mu)^2.\end{split}\]</div>

<div class="slide-no">14</div>

</article>
<article class="appear slide level-2" id="id7">
<h2>MLE Example</h2>
<ul class="simple">
<li>The MLE, <span class="math">\(\hat{\mu}\)</span>, is the value that sets <span class="math">\(\frac{d}{d
\mu} \ell(\mu|{\bf y}) = 0\)</span>:</li>
</ul>
<div class="to-build math">
\[\frac{d}{d \mu} \ell(\mu|{\bf y}) \bigg|_{\hat{\mu}} =
\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \hat{\mu}) = 0\]</div>
<div class="to-build math">
\[\Rightarrow \sum_{i=1}^n (y_i - \hat{\mu}) = 0\]</div>
<div class="to-build math">
\[\Rightarrow \sum_{i=1}^n \hat{\mu} = \sum_{i=1}^n y_i\]</div>
<div class="to-build math">
\[\Rightarrow \hat{\mu} = \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i.\]</div>

<div class="slide-no">15</div>

</article>
<article class="appear slide level-2" id="mle-example-unknown">
<h2>MLE Example: <span class="math">\(n=1\)</span>, Unknown <span class="math">\(\mu\)</span></h2>
<p>Suppose we have only one observation: <span class="math">\(y_1\)</span>.</p>
<ul class="to-build simple">
<li>If we specialize the previous result:</li>
</ul>
<div class="to-build math">
\[\begin{split}\hat{\mu} &amp; = y_1.\end{split}\]</div>
<ul class="to-build simple">
<li>The density <span class="math">\(f_{Y_1}(y_1|\mu)\)</span> gives the probability of
observing some data value <span class="math">\(y_1\)</span>, conditional on some <em>known</em>
parameter <span class="math">\(\mu\)</span>.</li>
</ul>
<ul class="to-build simple">
<li>This is a normal distribution with mean <span class="math">\(\mu\)</span> and variance
<span class="math">\(\sigma^2\)</span>.</li>
</ul>

<div class="slide-no">16</div>

</article>
<article class="appear slide level-2" id="id8">
<h2>MLE Example: <span class="math">\(n=1\)</span>, Unknown <span class="math">\(\mu\)</span></h2>
<ul class="simple">
<li>The likelihood <span class="math">\(\mathcal{L}(\mu|y_1)\)</span> gives the probability of
<span class="math">\(\mu\)</span>, conditional on some observed data value <span class="math">\(y_1\)</span>.</li>
</ul>
<ul class="to-build simple">
<li>This is a normal distribution with mean <span class="math">\(y_1\)</span> and variance
<span class="math">\(\sigma^2\)</span>.</li>
</ul>

<div class="slide-no">17</div>

</article>
<article class="appear slide level-2" id="id9">
<h2>MLE Example: <span class="math">\(n=1\)</span></h2>
<a class="reference internal image-reference" href="_images/densExample.png"><img alt="_images/densExample.png" class="align-center" src="_images/densExample.png" style="width: 7.5in;" /></a>

<div class="slide-no">18</div>

</article>
<article class="appear slide level-2" id="id10">
<h2>MLE Example: <span class="math">\(n=1\)</span>, Unknown <span class="math">\(\mu\)</span></h2>
<a class="reference internal image-reference" href="_images/likeExample.png"><img alt="_images/likeExample.png" class="align-center" src="_images/likeExample.png" style="width: 5in;" /></a>

<div class="slide-no">19</div>

</article>
<article class="appear slide level-2" id="id11">
<h2>MLE Example: <span class="math">\(n=1\)</span>, Unknown <span class="math">\(\sigma\)</span></h2>
<p>Let's continue with the assumption of one data observation, <span class="math">\(y_1\)</span>.</p>
<ul class="to-build simple">
<li>If <span class="math">\(\mu\)</span> is known but <span class="math">\(\sigma\)</span> is unknown, the density
of the data, <span class="math">\(y_1\)</span>, is still normal.</li>
</ul>
<ul class="to-build simple">
<li>However, the likelihood is</li>
</ul>
<div class="to-build math">
\[\mathcal{L}(\sigma^2|y_1) = \frac{\alpha}{\sigma^2}
\exp\left\{-\frac{\beta}{\sigma^2}\right\}\]</div>
<div class="to-build math">
\[\alpha = \frac{1}{\sqrt{2\pi}}, \qquad \beta =
\frac{(y_1-\mu)^2}{2}.\]</div>
<ul class="to-build simple">
<li>The likelihood for <span class="math">\(\sigma^2\)</span> is <em>not</em> normal, but <em>inverse
gamma</em>.</li>
</ul>

<div class="slide-no">20</div>

</article>
<article class="appear slide level-2" id="id12">
<h2>MLE Example: <span class="math">\(n=1\)</span>, Unknown <span class="math">\(\sigma\)</span></h2>
<a class="reference internal image-reference" href="_images/likeExample2.png"><img alt="_images/likeExample2.png" class="align-center" src="_images/likeExample2.png" style="width: 5in;" /></a>

<div class="slide-no">21</div>

</article>
<article class="appear slide level-2" id="mle-accuracy">
<h2>MLE Accuracy</h2>
<p>Maximum likelihood estimation results in estimates of true unknown
parameters.</p>
<ul class="to-build simple">
<li>What is the probability that our estimates are identical to the true
population parameters?</li>
</ul>
<ul class="to-build simple">
<li>Our estimates are imprecise and contain error.</li>
</ul>
<ul class="to-build simple">
<li>We would like to quantify the precision of our estimates with
standard errors.</li>
</ul>
<ul class="to-build simple">
<li>We will use the <em>Fisher Information</em> to compute standard errors.</li>
</ul>

<div class="slide-no">22</div>

</article>
<article class="appear slide level-2" id="fisher-information">
<h2>Fisher Information</h2>
<p>Suppose our likelihood is a function of a single parameter,
<span class="math">\(\theta\)</span>: <span class="math">\(\mathcal{L}(\theta|{\bf y})\)</span>.</p>
<ul class="to-build simple">
<li>The Fisher Information is</li>
</ul>
<div class="to-build math">
\[\begin{split}\mathcal{I}(\theta) &amp; = - E\left[\frac{d^2}{d \theta^2}
\ell(\theta|{\bf y}) \right].\end{split}\]</div>
<ul class="to-build simple">
<li>The observed Fisher Information is</li>
</ul>
<div class="to-build math">
\[\begin{split}\widetilde{\mathcal{I}}(\theta) &amp; = - \frac{d^2}{d \theta^2}
\ell(\theta|{\bf y}).\end{split}\]</div>

<div class="slide-no">23</div>

</article>
<article class="appear slide level-2" id="id13">
<h2>Fisher Information</h2>
<ul class="simple">
<li>Observed Fisher Information does not take an expectation, which may
be difficult to compute.</li>
</ul>
<ul class="to-build simple">
<li>Since <span class="math">\(\ell(\theta|{\bf y})\)</span> is often a sum of many terms,
<span class="math">\(\widetilde{\mathcal{I}}(\theta)\)</span> will converge to
<span class="math">\(\mathcal{I}(\theta)\)</span> for large samples.</li>
</ul>

<div class="slide-no">24</div>

</article>
<article class="appear slide level-2" id="mle-central-limit-theorem">
<h2>MLE Central Limit Theorem</h2>
<p>Under certain conditions, a central limit theorem holds for the MLE,
<span class="math">\(\hat{\theta}\)</span>.</p>
<ul class="to-build simple">
<li>For infinitely large samples <span class="math">\({\bf y}\)</span>,</li>
</ul>
<div class="to-build math">
\[\hat{\theta} \sim \mathcal{N}(\theta, \mathcal{I}(\theta)^{-1}).\]</div>
<ul class="to-build simple">
<li>For large samples, <span class="math">\(\hat{\theta}\)</span> is normally distributed
<em>regardless</em> of the distribution of the data, <span class="math">\({\bf y}\)</span>.</li>
</ul>

<div class="slide-no">25</div>

</article>
<article class="appear slide level-2" id="id14">
<h2>MLE Central Limit Theorem</h2>
<ul class="simple">
<li><span class="math">\(\hat{\theta}\)</span> is also normally distributed for large samples
even if <span class="math">\(\mathcal{L}(\theta|{\bf y})\)</span> is some other
distribution.</li>
</ul>
<ul class="to-build simple">
<li>Hence, for large samples,</li>
</ul>
<div class="to-build math">
\[\begin{split}Var(\hat{\theta}) &amp; = \frac{1}{\mathcal{I}(\theta)} \qquad
\Rightarrow \qquad Std(\hat{\theta}) =
\frac{1}{\sqrt{\mathcal{I}(\theta)}}.\end{split}\]</div>

<div class="slide-no">26</div>

</article>
<article class="appear slide level-2" id="mle-standard-errors">
<h2>MLE Standard Errors</h2>
<p>Since we don't know the true <span class="math">\(\theta\)</span>, we approximate</p>
<div class="math">
\[\begin{split}Std(\hat{\theta}) &amp; \approx
\frac{1}{\sqrt{\mathcal{I}(\hat{\theta})}}.\end{split}\]</div>
<ul class="to-build simple">
<li>Alternatively, to avoid computing the expectation, we could use the
approximation</li>
</ul>
<div class="to-build math">
\[\begin{split}Std(\hat{\theta}) &amp; \approx
\frac{1}{\sqrt{\widetilde{\mathcal{I}}(\hat{\theta})}}.\end{split}\]</div>

<div class="slide-no">27</div>

</article>
<article class="appear slide level-2" id="id15">
<h2>MLE Standard Errors</h2>
<ul class="simple">
<li>In reality, we never have an infinite sample size.</li>
</ul>
<ul class="to-build simple">
<li>For finite samples, these values are approximations of the standard
error of <span class="math">\(\hat{\theta}\)</span>.</li>
</ul>

<div class="slide-no">28</div>

</article>
<article class="appear slide level-2" id="mle-variance-example">
<h2>MLE Variance Example</h2>
<p>Let's return to the example where <span class="math">\(Y_1, \ldots, Y_n
\stackrel{i.i.d.}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span>, with known
<span class="math">\(\sigma\)</span>.</p>
<ul class="to-build simple">
<li>The log likelihood is</li>
</ul>
<div class="to-build math">
\[\begin{split}\ell(\mu|{\bf y}) &amp; = -\frac{n}{2} \log(2\pi) - \frac{n}{2}
\log(\sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \mu)^2.\end{split}\]</div>
<ul class="to-build simple">
<li>The resulting derivatives are</li>
</ul>
<div class="to-build math">
\[\begin{split}\frac{\partial \ell(\mu|{\bf y})}{\partial \mu} &amp; =
\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \mu),  \qquad
\frac{\partial^2 \ell(\mu|{\bf y})}{\partial \mu^2} =
-\frac{n}{\sigma^2}.\end{split}\]</div>

<div class="slide-no">29</div>

</article>
<article class="appear slide level-2" id="id16">
<h2>MLE Variance Example</h2>
<p>In this case the Fisher Information is identical to the observed
Fisher Information:</p>
<div class="math">
\[\begin{split}\mathcal{I}(\mu) &amp; = -E\left[-\frac{n}{\sigma^2}\right] =
\frac{n}{\sigma^2} = \widetilde{\mathcal{I}}(\mu).\end{split}\]</div>
<ul class="to-build simple">
<li>Since <span class="math">\(\mathcal{I}(\mu)\)</span> doesn't depend on <span class="math">\(\mu\)</span>, we
don't need to resort to an approximation with <span class="math">\(\hat{\mu} =
\bar{{\bf y}}\)</span>.</li>
</ul>
<ul class="to-build simple">
<li>The result is</li>
</ul>
<div class="to-build math">
\[\begin{split}Std(\hat{\mu})  &amp; = \frac{1}{\sqrt{\mathcal{I}(\mu)}} =
\frac{\sigma}{\sqrt{n}}.\end{split}\]</div>

<div class="slide-no">30</div>

</article>


</section>

    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     'Advanced Quantitative Methods',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/common.js"></script>
    <script type="text/javascript" src="_static/slides.js"></script>
    <script type="text/javascript" src="_static/sync.js"></script>
    <script type="text/javascript" src="_static/controller.js"></script>
    <script type="text/javascript" src="_static/init.js"></script>
    
  </body>
</html>
